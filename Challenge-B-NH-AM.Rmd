---
title: "Challenge B"
author: "Naïma Hamzaoui - Anne Messaoudene"
date: "Due date: 8th of December, 2017"
output: html_document
---
  
  URL of our repository: https://github.com/AnnneM/ChallengeB, 


```{r setup, include=FALSE}

knitr::opts_chunk$set(warning = FALSE)

```

```{r, message=FALSE, warning=FALSE, include=FALSE}

load.libraries <- c('readxl','np','tidyverse', 'randomForest', 'car', 'caret', 'knitr', 'stringr', 'dplyr')
install.lib <- load.libraries[!load.libraries %in% installed.packages()]
for(libs in install.lib) install.packages(libs, dependencies = TRUE)
sapply(load.libraries, require, character = TRUE)

```

# Task 1B - Predicting house prices in Ames, Iowa (continued)

### Step 1

We are going to use the Random Forests method. It's a tree based method, random forests represent a way in which to combine the predictions of multiple trees to form one combined prediction. It is linked to the method of bagging. The essential idea in bagging is to reduce variance in the trees by averaging them. The idea of random forest is to improve the variance reduction by reducing the correlation between the trees.


### Step 2

We train the Random Forests technique on the training data.

```{r, message=FALSE, warning=FALSE, include=FALSE}

rm(list=ls())

# we use the field separator character, sep
train <- read.table(file = "train.csv", header = TRUE, sep = ",")

attach(train)

#We delete the missing observations as we did in the challenge A, we choose to your solutions.
#We begin by plotting them.

train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)

plot_Missing <- function(data_in, title = NULL){
temp_df <- as.data.frame(ifelse(is.na(data_in), 0, 1))
temp_df <- temp_df[,order(colSums(temp_df))]
data_temp <- expand.grid(list(x = 1:nrow(temp_df), y = colnames(temp_df)))
data_temp$m <- as.vector(as.matrix(temp_df))
data_temp <- data.frame(x = unlist(data_temp$x), y = unlist(data_temp$y), m = unlist(data_temp$m))
ggplot(data_temp) + geom_tile(aes(x=x, y=y, fill=factor(m))) + scale_fill_manual(values=c("white", "black"), name="Missing\n(0=Yes, 1=No)") + theme_light() + ylab("") + xlab("") + ggtitle(title)
}

plot_Missing(train[,colSums(is.na(train)) > 0])

#Then we remove the variables that have 100 missing observations or more and that are not too important to explain the sale price.

remove.vars <- train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 100) %>% select(feature) %>% unlist

train <- train %>% select(- one_of(remove.vars))

#We remove the missing observations for important variables.

train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)

train <- train %>% filter(is.na(GarageType) == FALSE, is.na(MasVnrType) == FALSE, is.na(BsmtFinType2) == FALSE, is.na(BsmtExposure) == FALSE, is.na(Electrical) == FALSE)

# We make sure it's all clean, and it is.  

train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)

```

```{r, message=FALSE, warning=FALSE, include=FALSE}

#We use the random forest technique.
training.rf = randomForest(SalePrice ~ MSSubClass + MSZoning + LotArea + Street + LotShape
                           + LandContour + Utilities + LotConfig + LandSlope + Neighborhood
                           + Condition1 + Condition2 + BldgType + HouseStyle + OverallQual
                           + OverallCond + YearBuilt + YearRemodAdd + RoofStyle + RoofMatl
                           + Exterior1st + Exterior2nd + MasVnrType + MasVnrArea + ExterQual
                           + ExterCond + Foundation + BsmtQual + BsmtCond + BsmtExposure
                           + BsmtFinType1 + BsmtFinType2 + BsmtFinSF1 + BsmtFinSF2
                           + BsmtUnfSF + TotalBsmtSF + Heating + HeatingQC + CentralAir
                           + Electrical + X1stFlrSF + X2ndFlrSF + LowQualFinSF + GrLivArea
                           + BsmtFullBath + BsmtHalfBath + FullBath + HalfBath
                           + BedroomAbvGr + KitchenAbvGr + KitchenQual + TotRmsAbvGrd
                           + Functional + Fireplaces + GarageType + GarageYrBlt
                           + GarageFinish + GarageCars + GarageArea + GarageQual
                           + GarageCond + PavedDrive + WoodDeckSF + OpenPorchSF
                           + EnclosedPorch + X3SsnPorch + ScreenPorch + PoolArea + MiscVal
                           + YrSold + SaleType + SaleCondition, data = train)
training.rf

```
Type of random forest: regression  
Number of trees: 500  
No. of variables tried at each split: 24  



### Step 3

We use the Random Forests model to make predictions on the test data.

```{r, message=FALSE, warning=FALSE, include=FALSE}


test <- read.table(file = "test.csv" , header = TRUE, sep = ",")


#We use the same technique as in the challenge A.
#We delete the missing observations.
#We begin by plotting them.

test %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)

plot_Missing <- function(data_in, title = NULL){
  temp_df <- as.data.frame(ifelse(is.na(data_in), 0, 1))
  temp_df <- temp_df[,order(colSums(temp_df))]
  data_temp <- expand.grid(list(x = 1:nrow(temp_df), y = colnames(temp_df)))
  data_temp$m <- as.vector(as.matrix(temp_df))
  data_temp <- data.frame(x = unlist(data_temp$x), y = unlist(data_temp$y), m = unlist(data_temp$m))
  ggplot(data_temp) + geom_tile(aes(x=x, y=y, fill=factor(m))) + scale_fill_manual(values=c("white", "black"), name="Missing\n(0=Yes, 1=No)") + theme_light() + ylab("") + xlab("") + ggtitle(title)
}

plot_Missing(test[,colSums(is.na(test)) > 0])

#Then we remove the variables that have 100 missing observations or more and that are not too important to explain the sale price.

remove.vars <- test %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 100) %>% select(feature) %>% unlist

test <- test %>% select(- one_of(remove.vars))

#Then we remove the missing observations for important variables as we did in challenge A.

test %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)

test <- test %>% filter(is.na(GarageType) == FALSE, is.na(MasVnrType) == FALSE, is.na(BsmtFinType2) == FALSE, is.na(BsmtExposure) == FALSE, is.na(Electrical) == FALSE)

#We make sure it's all clean, and it is.

test %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)

#We use our trained model from above to make predictions on the test dataset. 
```

```{r, echo=TRUE, message=FALSE, warning=FALSE, include=FALSE}
common <- intersect(names(train), names(test)) 
for (p in common) { 
  if (class(train[[p]]) == "factor") { 
    levels(test[[p]]) <- levels(train[[p]]) 
  } 
}

test.rf <- predict(object = training.rf, newdata = test, predict.all = FALSE)

```

Our sample linear regression model (we use the variable that were in your best regressions you made during the challenge A solutions). 

```{r, echo=TRUE, warning=FALSE, include=FALSE}

reg <- lm(SalePrice ~ MSZoning + LotArea + Neighborhood  + YearBuilt + OverallQual, data = train)
preds <- predict.lm(reg, test)
```
We plot the predictions given both by the Random Forests model and the simple linear regression model.

```{r, echo=TRUE, message=FALSE, warning=FALSE}

ggplot() + geom_line(aes(x = test$Id, y = preds), colour = "red") + geom_point(aes(x = test$Id, y = test.rf), colour = "blue", size = 1)

```

When we compare the predictions of our linear regression and the predictions on the dataset we see that both predictions follow the same train. Random Forests, model in blue dots and the linear model in red line. We cannot see a difference between the two variances.  


# Task 2B - Overfitting in Machine Learning (continued)

```{r, message=FALSE, warning=FALSE, include=FALSE}

#Previously, in "Overfitting in Machine Learning"

rm(list=ls())

set.seed(1)
Nsim <- 150
b <- c(0,1)
x0 <- rep(1, Nsim)
x1 <- rnorm(n = Nsim)

X <- cbind(x0, x1^3)
y.true <- X %*% b

eps <- rnorm(n = Nsim)
y <- X %*% b + eps

dataframe <- tbl_df(y[,1]) %>% rename(y = value) %>% bind_cols(tbl_df(x1)) %>% rename(x = value) %>% bind_cols(tbl_df(y.true[,1])) %>% rename(y.true = value)

training.index <- createDataPartition(y = y, times = 1, p = 0.8)
dataframe <- dataframe %>% mutate(which.data = ifelse(1:n() %in% training.index$Resample1, "trainingB", "testB"))

trainingB <- dataframe %>% filter(which.data == "trainingB")
testB <- dataframe %>% filter(which.data == "testB")

```

### Step 1

```{r, echo=TRUE, message=FALSE, warning=FALSE}

ll.fit.lowflex <- npreg(y ~ x, data = trainingB, method = "ll", bws = 0.5)
summary(ll.fit.lowflex)

```

### Step 2


```{r, echo=TRUE, message=FALSE, warning=FALSE}

ll.fit.highflex <- npreg(y ~ x, data = trainingB, method = "ll", bws = 0.01)
summary(ll.fit.highflex)

dataframe <- dataframe %>% mutate(y.ll.lowflex = predict(object = ll.fit.lowflex, newdata = dataframe), y.ll.highflex = predict(object = ll.fit.highflex, newdata = dataframe))

trainingB <- trainingB %>% mutate(y.ll.lowflex = predict(object = ll.fit.lowflex, newdata = trainingB), y.ll.highflex = predict(object = ll.fit.highflex, newdata = trainingB))

```

### Step 3

```{r, echo=TRUE, message=FALSE, warning=FALSE}

ggplot(data=trainingB) + geom_point(aes(x,y)) + geom_line(aes(x=x,y=x^3), colour = "red" ) + geom_line(aes(x=x, y=y.ll.highflex), colour = "blue") + geom_line(aes(x=x, y=y.ll.lowflex), colour = "green")

```
The following graph depicts :
- The observations from the test dataset (black dots)
- The true model (red line)
- The predictions given by the high flexibility local linear model (blue line)
- The predictions given by the low flexibility local linear model (green line)


### Step 4

Between the two models we can see that the model with high flexibility (the blue line) is the one in which predictions are more variable because the graph show that the line is not smooth.
The predictions which have the least bias is the low flexibility model (green line) because we see that the green line has the same trend as the red line which is the true model. 

### Step 5

```{r, echo=TRUE, message=FALSE, warning=FALSE}

testB <- testB %>% mutate(y.ll.lowflex = predict(object = ll.fit.lowflex, newdata = testB), y.ll.highflex = predict(object = ll.fit.highflex, newdata = testB))

test.hf <- predict(object = ll.fit.highflex, x = testB$x, y = testB$y, se.fit = FALSE, level = 0.95)
test.lf <- predict(ll.fit.lowflex, newdata = testB, se.fit = FALSE, level = 0.95)

ggplot(data=testB) + geom_line(aes(x=testB$x,y=y.ll.highflex), colour="blue") +             geom_line(aes(x=testB$x,y=y.ll.lowflex),colour="green") + geom_point(aes(x=testB$x,y=testB$y), colour="black") +
geom_line(aes(x=x,y=x^3), colour = "red" )

```
The following graph depicts :  
- The observations from the test dataset (black dots)  
- The true model (red line)  
- The predictions given by the high flexibility local linear model (blue line)  
- The predictions given by the low flexibility local linear model (green line)

Between the two models we can see that the model with high flexibility (the blue line) is the one in which predictions are more variable because the graph show that the line is not smooth.
The gaps between the green line (the least biased model) and the observations (black dots) is smaller than with the blue line. 

### Step 6

```{r vector, echo= TRUE, message=FALSE, warning=FALSE, include=FALSE}

vector <- seq(from = 0.01, to = 0.5, by = 0.001)
length(vector)

```
It has a length equal to 491.

### Step 7

```{r, echo=TRUE, message=FALSE, warning=FALSE, include=FALSE}
llvector.fit <- lapply(X = vector, FUN = function(vector) {npreg(y ~ x, data = trainingB, method = "ll", bws = vector)})

```
For each bandwhidth, we obtain 491 different models. 

### Step 8

```{r, echo=TRUE, message=FALSE, warning=FALSE, include=FALSE}

mse.training <- function(fit.model){
predictions <- predict(object = fit.model, newdata = trainingB)
trainingB %>% mutate(squared.error = (y - predictions)^2) %>% summarize(mse = mean(squared.error))
}

mse.train.results <- unlist(lapply(X = llvector.fit, FUN = mse.training))

```
For each model, we obtain a vector of 491 different MSE.

### Step 9

```{r, echo=TRUE, message=FALSE, warning=FALSE, include=FALSE}

mse.test <- function(fit.model){
predictions <- predict(object = fit.model, newdata = testB)
testB %>% mutate(squared.error = (y - predictions)^2) %>% summarize(mse = mean(squared.error))
}
mse.test.results <- unlist(lapply(X = llvector.fit, FUN = mse.test))

```

### Step 10

```{r, echo=TRUE, message=FALSE, warning=FALSE}
mse.dataframe <- tbl_df(data.frame(bandwidth = vector, mse.train = mse.train.results, mse.test = mse.test.results))

ggplot(mse.dataframe) + 
geom_line(mapping = aes(x = vector, y = mse.train), color = "pink") +
geom_line(mapping = aes(x = vector, y = mse.test), color = "blue")

```

The pink line is the MSEs of the training data, whereas the blue line the MSEs test data, as the bandwidth increases. The blue one represents test MSE and pink line represents the training MSE. The vector shows flexibility. The Mean squared error is equal to zero when the model is completely flexible (there is no gap, it connects all the dot). 
Also, we see that the curves are smoother, it show that when the bandwidth increases, the MSE increases. The test MSE initially declines as the level of flexibility increases. However at some points the test MSE levels off and then start to increase again.  
When the slope is equal to zero it is the optimal bandwidth when the MSE is minimised. The MSE is very large when the bandwidth is low because of the bias. 
When the given method yields a small training MSE but a large test MSE, we are said to overfitting the data. 
 

# Task 3B - Privacy regulation compliance in France





### Step 1

```{r, echo=TRUE, message=FALSE, warning=FALSE}

rm(list=ls())
CIL <- read.csv(file="~/rprog/CIL.csv", header=TRUE, sep =",")

attach(CIL)

```

### Step 2

```{r, echo= TRUE, eval=FALSE, include=FALSE}

View(CIL)

```

```{r, echo=TRUE, message=FALSE, warning=FALSE}

CIL2 <- read.csv(file="~/Desktop/rprog/CIL2.csv", header=TRUE, sep=";")
attach(CIL2)
dept <- str_sub(Code_Postal, start = 1, end = 2)

kable(as.data.frame(table(dept)))

```

### Step 3

With help (on the page amunategui.github.io/dealing-with-large-files/), we saw that it was possible to fit this huge fil in our computer live memory using data in chunks. We have a problem when we run it : 
Error in type.convert(data[[i]], as.is = as.is[i], dec = dec, numerals = numerals,  : 
  chaîne de charactères multioctets incorrecte à '<e0> 19<39> salari<e9>s'
  
We tried to use fileEnoding = "latin1" but the problem wasn't resolve.
Then we delete 12 observations (inverted code postale with city name) but the same error message appeared. We think there are missing observations and misreported code postal (for example the code postal 03100 is written 3100 which represent the departement 31 instead of 03). 

We can not detail the time needed to run the steps in this task because of previous error message. 

```{r, echo=TRUE, message=FALSE, warning=FALSE, include=FALSE}

SIRENFILE <- "SIREN.csv"
index <- 0
chunksize <- 100000 
con <- file(description = SIRENFILE, open="r") 
datachunk <- read.table(con, nrows = chunksize, header = TRUE, fill = TRUE, sep = ";")
actualcolnames <- names(datachunk) 

```

```{r,echo=TRUE, message=FALSE, warning=FALSE, include=FALSE}

system.time(repeat{
  index <- index +  1
  print(paste('Processing rows:', index * chunksize))
  
  MERGEDDATA <- merge(x = CIL2, y = datachunk, by.x = "Siren", by.y = "SIREN",
                      all.x = FALSE, all.y = FALSE)
  
  if (nrow(datachunk) !=chunksize){
  
    print('Processed all chunks!')
    break} 
  
  datachunk <- read.table(con, nrows=chunksize, skip=0, header = FALSE, fill = TRUE,
                          sep = ";", col.names = actualcolnames)

})

close(con)

```

```{r, echo=TRUE}

nrow(MERGEDDATA)

```

```{r, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}

SIRENFILE <- "SIREN.csv"
index <- 0
chunksize <- 100000
con <- file(description = SIRENFILE, open="r")
datachunk <- read.table(con, nrows = chunksize, header = TRUE, fill = TRUE, sep = ";")
actualcolnames <- names(datachunk)

repeat{
index <- index +  1
print(paste('Processing rows:', index * chunksize))

attach(datachunk)
SIRENFILE$SIREN2 <- str_sub(SIREN, start = 1, end = 9)

if (nrow(datachunk) !=chunksize){
print('Processed all files!')
break} 

datachunk <- read.table(con, nrows=chunksize, skip=0, header = FALSE, fill = TRUE,
                        sep = ";", col.names = actualcolnames)

break
}

close(con)

```

### Step 4

To answer the questions of the step4, we choose to use the variable EFENCENT, it represents the number of employees associated to each siren number. 

```{r, echo=TRUE, message=FALSE, warning=FALSE}

attach(MERGEDDATA)

ggplot(MERGEDDATA[which(MERGEDDATA$EFENCENT != "NN"),]) + 
  geom_bar(mapping = aes(x = EFENCENT), color = "blue", fill = "blue")

```

The graph doesn't give us real information about the size of company that noominated the CIL because one compagny can have several subsidiaries appears several times in the data and so there are over represented in the histogram. 

